# ğŸ¤– Simple Computer Use

Control your computer with natural language commands using OCR Large Language Models (LLMs). Tested on Windows 10 and Ubuntu 22.04 LTS.

Inspired from the amazing [OmniParser](https://github.com/microsoft/OmniParser) by Microsoft.

[demo.webm](https://github.com/user-attachments/assets/bdd5bc25-fe88-4105-a3ed-f435f98e4f18)

## âœ¨ Features

- ğŸ—£ï¸ **Natural Language Commands**: Control your computer using everyday language
- ğŸ” **UI Element Detection**: Automatically detects UI elements on your screen
- ğŸ“ **Multi-Step Commands**: Execute complex sequences of actions with a single command
- ğŸ‘ï¸ **OCR Integration**: Reads text from your screen to better understand the context
- âŒ¨ï¸ **Keyboard and Mouse Control**: Simulates keyboard and mouse actions

## ğŸš€ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-pc-control.git
cd llm-pc-control

# Install the package
pip install -e .
```

## ğŸ“‹ Requirements

- Python 3.8 or higher
- Ollama (for local LLM inference)
- EasyOCR and PaddleOCR (for text recognition)
- PyAutoGUI (for keyboard and mouse control)

## ğŸ“– Usage

### ğŸ’» Command Line Interface

```bash
# Set up the environment (download models, check dependencies)
llm-pc-control setup

# Run a single command
llm-pc-control run "click on the button"

# Run in interactive mode
llm-pc-control interactive
```

### ğŸ Python API

```python
from llm_control.main import setup, run_command

# Set up the environment
setup()

# Run a command
run_command("click on the button")
```

## ğŸ’¡ Examples

Here are some examples of commands you can use:

- "Click on the Submit button"
- "Type 'Hello, world!' in the search box"
- "Press Enter"
- "Move to the top-right corner of the screen"
- "Double-click on the file icon"
- "Right-click on the image"
- "Scroll down"
- "Click on the button, then type 'Hello', then press Enter"

## âš™ï¸ How It Works

1. ğŸ“¸ **Screenshot Analysis**: Takes a screenshot of your screen
2. ğŸ” **UI Detection**: Analyzes the screenshot to detect UI elements
3. ğŸ”„ **Command Parsing**: Parses your natural language command into steps
4. âš¡ **Action Generation**: Generates the corresponding actions for each step
5. â–¶ï¸ **Execution**: Executes the actions using PyAutoGUI

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
