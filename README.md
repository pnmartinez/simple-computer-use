# ğŸ¤– LLM PC Control

[![Voice Server Verification](https://github.com//pnmartinez/simple-computer-use/actions/workflows/voice-server-verification.yml/badge.svg)](https://github.com/pnmartinez/simple-computer-use/actions/workflows/voice-server-verification.yml)

> For voice control with phone check: https://github.com/pnmartinez/computer-use-android-app

[demo.webm](https://github.com/user-attachments/assets/bdd5bc25-fe88-4105-a3ed-f435f98e4f18)

Control your computer with natural language commands using Large Language Models (LLMs), OCR, and voice input.

Get the Android app to control your PC with voice on the [Computer Use Android App](https://github.com/pnmartinez/computer-use-android-app) repo.

![imagen](https://github.com/user-attachments/assets/c9e3408e-b708-4633-bf4e-a6698cb1cee1)


## âœ¨ Features

- ğŸ—£ï¸ **Natural Language Commands**: Control your computer using everyday language
- ğŸ” **UI Element Detection**: Automatically detects UI elements on your screen
- ğŸ“ **Multi-Step Commands**: Execute complex sequences of actions with a single command
- ğŸ‘ï¸ **OCR Integration**: Reads text from your screen to better understand the context
- âŒ¨ï¸ **Keyboard and Mouse Control**: Simulates keyboard and mouse actions
- ğŸ¤ **Voice Input Support**: Control your PC with voice commands
- ğŸŒ **Multilingual Support**: Automatic translation with preservation of UI element names
- ğŸ–¥ï¸ **Multiple Deployment Options**: Run locally or in Docker

## ğŸš€ Installation

### Standard Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-pc-control.git
cd llm-pc-control

# Install the package
pip install -e .
```

### Docker Installation

For a Docker-based setup:

1. Make sure Docker and Docker Compose are installed
2. Ensure [Ollama](https://ollama.ai/) is installed and running locally
3. Run the setup script:

```bash
./scripts/docker/setup-docker-x11.sh
```

## ğŸ“‹ Requirements

- Python 3.8 or higher
- Ollama (for local LLM inference)
- EasyOCR and PaddleOCR (for text recognition)
- PyAutoGUI (for keyboard and mouse control)
- PyAudio (for voice input)
- OpenAI Whisper (for speech-to-text)

## ğŸ“– Usage

### Voice Control Server

```bash
# Run the voice control server
python -m llm_control voice-server

# With custom options
python -m llm_control voice-server --port 8080 --whisper-model medium --ollama-model llama3.1
```

### Simple Command

```bash
# Run a simple command
python -m llm_control simple-voice --command "click on the Firefox icon"
```

## ğŸ–¥ï¸ Server API

The voice control server provides the following API endpoints:

- **GET /health**: Check server status
- **POST /command**: Execute a text command
- **POST /voice-command**: Process a voice command from audio data
- **POST /transcribe**: Transcribe audio without executing commands
- **POST /translate**: Translate text to English

### Example: Sending a Direct Command

```bash
curl -X POST http://localhost:5000/command \
  -H "Content-Type: application/json" \
  -d '{"command": "open Firefox, go to gmail.com and compose a new email"}'
```

### Example: Sending a Voice Command

```bash
curl -X POST http://localhost:5000/voice-command \
  -F "audio_file=@recording.wav" \
  -F "translate=true" \
  -F "language=es"
```

## ğŸ§ª Project Structure

```
llm-control/
â”œâ”€â”€ llm_control/         # Main Python package
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ docker/          # Docker-related scripts
â”‚   â”œâ”€â”€ setup/           # Installation scripts
â”‚   â””â”€â”€ tools/           # Utility tools
â”œâ”€â”€ data/                # Data files
â”œâ”€â”€ tests/               # Test suite
â””â”€â”€ screenshots/         # Screenshots directory
```

## ğŸ’¡ Command Examples

Here are some examples of commands you can use:

- "Click on the Submit button"
- "Type 'Hello, world!' in the search box"
- "Press Enter"
- "Move to the top-right corner of the screen"
- "Double-click on the file icon"
- "Right-click on the image"
- "Scroll down"
- "Click on the button, then type 'Hello', then press Enter"

## âš™ï¸ How It Works

1. ğŸ“¸ **Screenshot Analysis**: Takes a screenshot of your screen
2. ğŸ” **UI Detection**: Analyzes the screenshot to detect UI elements
3. ğŸ”„ **Command Parsing**: Parses your natural language command into steps
4. âš¡ **Action Generation**: Generates the corresponding actions for each step
5. â–¶ï¸ **Execution**: Executes the actions using PyAutoGUI

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
